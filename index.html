<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Robot Foundation Models Tracker</title>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600;700&family=Space+Grotesk:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg-primary: #0a0a0f;
            --bg-secondary: #12121a;
            --bg-card: #1a1a24;
            --bg-hover: #22222e;
            --accent-primary: #00ff88;
            --accent-secondary: #00ccff;
            --accent-tertiary: #ff6b35;
            --text-primary: #ffffff;
            --text-secondary: #a0a0b0;
            --text-muted: #606070;
            --border-color: #2a2a3a;
            --gradient-1: linear-gradient(135deg, #00ff88 0%, #00ccff 100%);
            --gradient-2: linear-gradient(135deg, #ff6b35 0%, #ff3366 100%);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Space Grotesk', sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            min-height: 100vh;
            overflow-x: hidden;
        }

        /* Animated background grid */
        .bg-grid {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background-image: 
                linear-gradient(rgba(0, 255, 136, 0.03) 1px, transparent 1px),
                linear-gradient(90deg, rgba(0, 255, 136, 0.03) 1px, transparent 1px);
            background-size: 50px 50px;
            pointer-events: none;
            z-index: 0;
        }

        .container {
            position: relative;
            z-index: 1;
            max-width: 1600px;
            margin: 0 auto;
            padding: 2rem;
        }

        /* Header */
        header {
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--border-color);
        }

        .header-top {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            margin-bottom: 1.5rem;
        }

        .logo-section h1 {
            font-size: 2.5rem;
            font-weight: 700;
            background: var(--gradient-1);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            letter-spacing: -0.02em;
        }

        .logo-section .subtitle {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.85rem;
            color: var(--text-muted);
            margin-top: 0.5rem;
        }

        .stats-bar {
            display: flex;
            gap: 2rem;
        }

        .stat-item {
            text-align: right;
        }

        .stat-value {
            font-family: 'JetBrains Mono', monospace;
            font-size: 1.75rem;
            font-weight: 700;
            color: var(--accent-primary);
        }

        .stat-label {
            font-size: 0.75rem;
            color: var(--text-muted);
            text-transform: uppercase;
            letter-spacing: 0.1em;
        }

        /* Filters */
        .filters {
            display: flex;
            gap: 1rem;
            flex-wrap: wrap;
            align-items: center;
        }

        .filter-group {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .filter-label {
            font-size: 0.75rem;
            color: var(--text-muted);
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        .filter-btn {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.8rem;
            padding: 0.5rem 1rem;
            background: var(--bg-secondary);
            border: 1px solid var(--border-color);
            color: var(--text-secondary);
            border-radius: 4px;
            cursor: pointer;
            transition: all 0.2s ease;
        }

        .filter-btn:hover {
            background: var(--bg-hover);
            border-color: var(--accent-primary);
            color: var(--text-primary);
        }

        .filter-btn.active {
            background: var(--accent-primary);
            border-color: var(--accent-primary);
            color: var(--bg-primary);
        }

        .search-input {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.85rem;
            padding: 0.5rem 1rem;
            background: var(--bg-secondary);
            border: 1px solid var(--border-color);
            color: var(--text-primary);
            border-radius: 4px;
            width: 250px;
            transition: all 0.2s ease;
        }

        .search-input:focus {
            outline: none;
            border-color: var(--accent-primary);
            box-shadow: 0 0 0 3px rgba(0, 255, 136, 0.1);
        }

        .search-input::placeholder {
            color: var(--text-muted);
        }

        /* Timeline View */
        .timeline-container {
            margin-bottom: 3rem;
            padding: 1.5rem;
            background: var(--bg-secondary);
            border-radius: 8px;
            border: 1px solid var(--border-color);
            overflow-x: auto;
        }

        .timeline {
            display: flex;
            gap: 0;
            min-width: max-content;
            position: relative;
            padding: 1rem 0;
        }

        .timeline::before {
            content: '';
            position: absolute;
            top: 50%;
            left: 0;
            right: 0;
            height: 2px;
            background: var(--border-color);
            transform: translateY(-50%);
        }

        .timeline-year {
            display: flex;
            flex-direction: column;
            align-items: center;
            padding: 0 1rem;
            position: relative;
        }

        .timeline-year-label {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.75rem;
            color: var(--accent-primary);
            margin-bottom: 1rem;
            font-weight: 600;
        }

        .timeline-dots {
            display: flex;
            gap: 0.5rem;
        }

        .timeline-dot {
            width: 12px;
            height: 12px;
            border-radius: 50%;
            cursor: pointer;
            transition: all 0.2s ease;
            position: relative;
        }

        .timeline-dot.vla { background: var(--accent-primary); }
        .timeline-dot.vam { background: var(--accent-tertiary); }
        .timeline-dot.world-model { background: var(--accent-secondary); }
        .timeline-dot.planner { background: #9966ff; }

        .timeline-dot:hover {
            transform: scale(1.5);
            box-shadow: 0 0 10px currentColor;
        }

        .timeline-dot::after {
            content: attr(data-name);
            position: absolute;
            bottom: 100%;
            left: 50%;
            transform: translateX(-50%);
            font-size: 0.7rem;
            white-space: nowrap;
            opacity: 0;
            transition: opacity 0.2s;
            padding: 0.25rem 0.5rem;
            background: var(--bg-card);
            border-radius: 4px;
            margin-bottom: 0.5rem;
        }

        .timeline-dot:hover::after {
            opacity: 1;
        }

        /* Legend */
        .legend {
            display: flex;
            gap: 1.5rem;
            margin-top: 1rem;
            justify-content: center;
        }

        .legend-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.75rem;
            color: var(--text-secondary);
        }

        .legend-dot {
            width: 10px;
            height: 10px;
            border-radius: 50%;
        }

        /* Cards Grid */
        .cards-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(400px, 1fr));
            gap: 1.5rem;
        }

        .model-card {
            background: var(--bg-card);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 1.5rem;
            transition: all 0.3s ease;
            cursor: pointer;
            position: relative;
            overflow: hidden;
        }

        .model-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 4px;
            height: 100%;
            background: var(--accent-primary);
            opacity: 0;
            transition: opacity 0.3s ease;
        }

        .model-card:hover {
            border-color: var(--accent-primary);
            transform: translateY(-2px);
            box-shadow: 0 10px 40px rgba(0, 255, 136, 0.1);
        }

        .model-card:hover::before {
            opacity: 1;
        }

        .model-card.vla::before { background: var(--accent-primary); }
        .model-card.vam::before { background: var(--accent-tertiary); }
        .model-card.world-model::before { background: var(--accent-secondary); }
        .model-card.planner::before { background: #9966ff; }

        .card-header {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            margin-bottom: 1rem;
        }

        .model-name {
            font-size: 1.25rem;
            font-weight: 600;
            color: var(--text-primary);
        }

        .model-org {
            font-size: 0.8rem;
            color: var(--text-muted);
            margin-top: 0.25rem;
        }

        .model-date {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.75rem;
            color: var(--accent-primary);
            background: rgba(0, 255, 136, 0.1);
            padding: 0.25rem 0.5rem;
            border-radius: 4px;
        }

        .card-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-bottom: 1rem;
        }

        .meta-tag {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.7rem;
            padding: 0.25rem 0.5rem;
            background: var(--bg-secondary);
            border: 1px solid var(--border-color);
            border-radius: 4px;
            color: var(--text-secondary);
        }

        .meta-tag.category {
            background: rgba(0, 204, 255, 0.1);
            border-color: var(--accent-secondary);
            color: var(--accent-secondary);
        }

        .card-insight {
            font-size: 0.85rem;
            color: var(--text-secondary);
            line-height: 1.6;
            margin-bottom: 1rem;
            display: -webkit-box;
            -webkit-line-clamp: 3;
            -webkit-box-orient: vertical;
            overflow: hidden;
        }

        .card-links {
            display: flex;
            gap: 0.75rem;
            flex-wrap: wrap;
        }

        .card-link {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.7rem;
            color: var(--accent-primary);
            text-decoration: none;
            padding: 0.25rem 0.5rem;
            border: 1px solid var(--accent-primary);
            border-radius: 4px;
            transition: all 0.2s ease;
        }

        .card-link:hover {
            background: var(--accent-primary);
            color: var(--bg-primary);
        }

        /* Modal */
        .modal-overlay {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.8);
            backdrop-filter: blur(4px);
            z-index: 1000;
            display: none;
            align-items: center;
            justify-content: center;
            padding: 2rem;
        }

        .modal-overlay.active {
            display: flex;
        }

        .modal {
            background: var(--bg-card);
            border: 1px solid var(--border-color);
            border-radius: 12px;
            max-width: 800px;
            width: 100%;
            max-height: 90vh;
            overflow-y: auto;
            padding: 2rem;
            position: relative;
        }

        .modal-close {
            position: absolute;
            top: 1rem;
            right: 1rem;
            background: none;
            border: none;
            color: var(--text-muted);
            font-size: 1.5rem;
            cursor: pointer;
            transition: color 0.2s;
        }

        .modal-close:hover {
            color: var(--text-primary);
        }

        .modal-header {
            margin-bottom: 1.5rem;
            padding-bottom: 1rem;
            border-bottom: 1px solid var(--border-color);
        }

        .modal-title {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
        }

        .modal-subtitle {
            color: var(--text-muted);
        }

        .modal-section {
            margin-bottom: 1.5rem;
        }

        .modal-section-title {
            font-size: 0.75rem;
            text-transform: uppercase;
            letter-spacing: 0.1em;
            color: var(--accent-primary);
            margin-bottom: 0.5rem;
        }

        .modal-section-content {
            color: var(--text-secondary);
            line-height: 1.6;
        }

        .modal-specs {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 1rem;
        }

        .spec-item {
            background: var(--bg-secondary);
            padding: 1rem;
            border-radius: 8px;
        }

        .spec-label {
            font-size: 0.7rem;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 0.25rem;
        }

        .spec-value {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.9rem;
            color: var(--text-primary);
        }

        /* Loading State */
        .loading {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            padding: 4rem;
            color: var(--text-muted);
        }

        .loading-spinner {
            width: 40px;
            height: 40px;
            border: 3px solid var(--border-color);
            border-top-color: var(--accent-primary);
            border-radius: 50%;
            animation: spin 1s linear infinite;
            margin-bottom: 1rem;
        }

        @keyframes spin {
            to { transform: rotate(360deg); }
        }

        /* Error State */
        .error {
            background: rgba(255, 107, 53, 0.1);
            border: 1px solid var(--accent-tertiary);
            border-radius: 8px;
            padding: 2rem;
            text-align: center;
            color: var(--accent-tertiary);
        }

        .error-title {
            font-size: 1.25rem;
            margin-bottom: 0.5rem;
        }

        .error-message {
            font-size: 0.9rem;
            opacity: 0.8;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            .header-top {
                flex-direction: column;
                gap: 1.5rem;
            }

            .stats-bar {
                width: 100%;
                justify-content: space-between;
            }

            .stat-item {
                text-align: center;
            }

            .cards-grid {
                grid-template-columns: 1fr;
            }

            .filters {
                flex-direction: column;
                align-items: stretch;
            }

            .search-input {
                width: 100%;
            }
        }
    </style>
</head>
<body>
    <div class="bg-grid"></div>
    
    <div class="container">
        <header>
            <div class="header-top">
                <div class="logo-section">
                    <h1>Robot Foundation Models</h1>
                    <p class="subtitle">// Tracking the evolution of embodied AI</p>
                </div>
                <div class="stats-bar">
                    <div class="stat-item">
                        <div class="stat-value" id="total-models">--</div>
                        <div class="stat-label">Models</div>
                    </div>
                    <div class="stat-item">
                        <div class="stat-value" id="total-orgs">--</div>
                        <div class="stat-label">Organizations</div>
                    </div>
                    <div class="stat-item">
                        <div class="stat-value" id="date-range">--</div>
                        <div class="stat-label">Timeline</div>
                    </div>
                </div>
            </div>

            <div class="filters">
                <div class="filter-group">
                    <span class="filter-label">Category:</span>
                    <button class="filter-btn active" data-filter="all">All</button>
                    <button class="filter-btn" data-filter="VLA">VLA</button>
                    <button class="filter-btn" data-filter="VAM">VAM</button>
                    <button class="filter-btn" data-filter="Video World Model">World Model</button>
                </div>
                <input type="text" class="search-input" placeholder="Search models, orgs, insights..." id="search">
            </div>
        </header>

        <div class="timeline-container">
            <div class="timeline" id="timeline"></div>
            <div class="legend">
                <div class="legend-item">
                    <div class="legend-dot" style="background: var(--accent-primary)"></div>
                    <span>VLA</span>
                </div>
                <div class="legend-item">
                    <div class="legend-dot" style="background: var(--accent-tertiary)"></div>
                    <span>VAM</span>
                </div>
                <div class="legend-item">
                    <div class="legend-dot" style="background: var(--accent-secondary)"></div>
                    <span>World Model</span>
                </div>
                <div class="legend-item">
                    <div class="legend-dot" style="background: #9966ff"></div>
                    <span>LLM Planner</span>
                </div>
            </div>
        </div>

        <div class="cards-grid" id="cards-grid">
            <div class="loading">
                <div class="loading-spinner"></div>
                <p>Loading models...</p>
            </div>
        </div>
    </div>

    <!-- Modal -->
    <div class="modal-overlay" id="modal-overlay">
        <div class="modal" id="modal">
            <button class="modal-close" onclick="closeModal()">&times;</button>
            <div id="modal-content"></div>
        </div>
    </div>

    <script>
        // ============================================
        // CONFIGURATION - UPDATE THIS WITH YOUR SHEET
        // ============================================
        
        // Option 1: Google Sheets (recommended)
        // 1. Create a new Google Sheet
        // 2. Import the CSV file (File > Import)
        // 3. Share the sheet (File > Share > Anyone with link can view)
        // 4. Copy the sheet ID from the URL (the long string between /d/ and /edit)
        // 5. Replace 'YOUR_SHEET_ID_HERE' below
        
        const GOOGLE_SHEET_ID = '1bXBH0CipQMYT14OC_YD0rQHEbtQm3xx7sTqbcHvMg9A';
        const SHEET_NAME = 'robot_foundation_models'; // Change if your sheet has a different name
        
        // Option 2: Use embedded data (fallback if no sheet configured)
        const USE_EMBEDDED_DATA = GOOGLE_SHEET_ID === '1bXBH0CipQMYT14OC_YD0rQHEbtQm3xx7sTqbcHvMg9A';
        
        // Embedded data (your current dataset)
        const EMBEDDED_DATA = [
            {"id":1,"name":"SayCan","org":"Google Robotics","date":"Apr 2022","category":"LLM Planner","backbone":"PaLM 540B + BC-Z + MT-Opt","params":"540B","decoder":"BC Policies","speed":"—","data":"68,000 teleoperated demos + 12,000 autonomous episodes; 10 robots over 11 months","insight":"Pre-end-to-end paradigm: modular hierarchy with LLM planning → skill selection → separate execution policies. First to ground LLMs in robotic affordances via value functions. Demonstrated LLM improvements translate to robotics (PaLM vs FLAN: 84% vs 70%). Superseded by end-to-end approaches (RT-1/RT-2) but hierarchical insight persists in later models.","paper_link":"https://arxiv.org/abs/2204.01691","website_link":"https://say-can.github.io/","github_link":"https://github.com/google-research/google-research/tree/master/saycan","blog_link":""},
            {"id":2,"name":"RT-1","org":"Google DeepMind","date":"Dec 2022","category":"VLA","backbone":"RT-1 (custom)","params":"35M","decoder":"Discrete Token Actions","speed":"3 Hz","data":"130k demos, 8 skills, 2 embodiments","insight":"First end-to-end robot foundation model. Precursor to modern VLM-based models. Custom transformer architecture for robotics control, not \"web-scale\".","paper_link":"https://arxiv.org/abs/2212.06817","website_link":"https://robotics-transformer1.github.io/","github_link":"https://github.com/google-research/robotics_transformer","blog_link":""},
            {"id":3,"name":"RT-2","org":"Google DeepMind","date":"Jul 2023","category":"VLA","backbone":"PaLI-X","params":"55B","decoder":"Discrete Token Actions","speed":"—","data":"Web-scale data + RT-1 Data","insight":"First true VLA. Established the paradigm of fine-tuning web-scale VLMs for robot control. Demonstrated web-scale pretraining transfers to robotics (62% vs 32% generalization over RT-1).","paper_link":"https://arxiv.org/abs/2307.15818","website_link":"https://robotics-transformer2.github.io/","github_link":"","blog_link":"https://deepmind.google/blog/rt-2-new-model-translates-vision-and-language-into-action/"},
            {"id":4,"name":"RT-H","org":"Google DeepMind","date":"Mar 2024","category":"VLA","backbone":"PaLI-X","params":"55B","decoder":"Discrete Token Actions","speed":"—","data":"RT-2 Data","insight":"First hierarchical VLA. Uses language motions as intermediate representation between task and actions. Foreshadows later System 1/System 2 architectures (Helix, π0.5). Enables human correction in language space. Outperforms RT-2 by 15%.","paper_link":"https://arxiv.org/abs/2403.01823","website_link":"https://rt-hierarchy.github.io/","github_link":"","blog_link":""},
            {"id":5,"name":"Octo","org":"UC Berkeley / Stanford / CMU","date":"May 2024","category":"VLA","backbone":"Octo (custom)","params":"93M","decoder":"Diffusion","speed":"—","data":"800k episodes, 11 embodiments (OpenX)","insight":"First VLA with diffusion-based action decoder. Enables continuous, multimodal action distributions vs rigid discrete tokens. First open-source generalist policy on OpenX (800k episodes, 11 embodiments). Diffusion later superseded by flow matching (π0).","paper_link":"https://arxiv.org/abs/2405.12213","website_link":"https://octo-models.github.io/","github_link":"https://github.com/octo-models/octo","blog_link":""},
            {"id":6,"name":"OpenVLA","org":"Stanford / Berkeley","date":"Jun 2024","category":"VLA","backbone":"Prismatic-7B (DINOv2 + SigLIP + Llama 2)","params":"7B","decoder":"Discrete Token Actions","speed":"3 Hz","data":"970k episodes from OpenX","insight":"First fully open-source VLA with training code. Outperforms RT-2-X (55B) by 16.5% with 7x fewer parameters (7B).","paper_link":"https://arxiv.org/abs/2406.09246","website_link":"https://openvla.github.io/","github_link":"https://github.com/openvla/openvla","blog_link":""},
            {"id":7,"name":"LAPA","org":"KAIST / UW / Microsoft / NVIDIA / Allen AI","date":"Oct 2024","category":"VLA","backbone":"Large World Model (LWM-Chat-1M)","params":"7B","decoder":"Discrete Action Tokens","speed":"—","data":"Pretrain: action-free video (BridgeV2 181k traj, Open-X, Something-Something V2 220K human videos). Fine-tune: ~100-1k labeled robot trajectories","insight":"Opens the action-free video era. First to learn robot control from video without action labels via VQ-VAE latent actions. 30x more efficient than OpenVLA pretraining. Approach adopted by GR00T N1 (latent action codebook) and subsequent models.","paper_link":"https://arxiv.org/abs/2410.11758","website_link":"https://lapa-vla.github.io/","github_link":"","blog_link":""},
            {"id":8,"name":"π0","org":"Physical Intelligence","date":"Oct 2024","category":"VLA","backbone":"PaliGemma + Action Expert","params":"3.6B","decoder":"Flow Matching","speed":"50 Hz","data":"10,000+ hrs, 7 robot configs","insight":"First VLA with flow matching action decoder (50 Hz vs ~3 Hz for discrete/diffusion approaches). Flow matching becomes dominant action decoder for subsequent VLAs (GR00T N1, GR00T N1.5, GR00T N1.6, π0.5, π0.6, mimic-video, 1XWM).","paper_link":"https://arxiv.org/abs/2410.24164","website_link":"https://www.physicalintelligence.company/blog/pi0","github_link":"https://github.com/Physical-Intelligence/openpi","blog_link":""},
            {"id":9,"name":"Cosmos WFM","org":"NVIDIA","date":"Jan 2025","category":"Video World Model","backbone":"Diffusion Transformer","params":"7B-14B","decoder":"—","speed":"0.4 FPS","data":"20 million hours of video","insight":"First large-scale video world models for robotics. Enables physics-aware synthetic training data generation to address robot data scarcity. Foundation for GR00T-Dreams and Cosmos-Predict2.5 synthetic data pipelines. Not a direct policy but post-trainable for action generation.","paper_link":"https://arxiv.org/abs/2501.03575","website_link":"https://www.nvidia.com/en-us/ai/cosmos/","github_link":"https://github.com/NVIDIA/Cosmos","blog_link":""},
            {"id":10,"name":"Helix","org":"Figure","date":"Feb 2025","category":"VLA","backbone":"7B VLM","params":"~7.1B","decoder":"Regression","speed":"7-9 Hz","data":"~500 hrs teleoperated data","insight":"First explicit System 1/System 2 architecture for VLAs: slow VLM (7-9 Hz) for semantic reasoning, fast transformer (200 Hz) for motor control. Architectural separation vs π0.5's single-model hierarchy. 200 Hz control enables reactive behaviors.","paper_link":"https://arxiv.org/abs/2502.20107","website_link":"","github_link":"","blog_link":"https://www.figure.ai/news/helix"},
            {"id":11,"name":"GR00T N1","org":"NVIDIA","date":"Mar 2025","category":"VLA","backbone":"Eagle-2 VLM","params":"2.2B","decoder":"Flow Matching","speed":"~16 Hz","data":"Human ego videos (Ego4D, EPIC-KITCHENS), Open X-Embodiment, AgiBot-Alpha, 88h GR-1 teleop, 827h neural trajectories, 780K sim trajectories","insight":"Latent action codebook (cf. LAPA) enables training on human ego video without action labels. First VLA spanning arms to humanoids (cross-embodiment). Human video (Ego4D, EPIC-KITCHENS) directly informs robot policy.","paper_link":"https://arxiv.org/abs/2503.14734","website_link":"","github_link":"https://github.com/NVIDIA-Omniverse/Isaac-GR00T","blog_link":"https://developer.nvidia.com/blog/nvidia-isaac-gr00t-n1-open-foundation-model-for-humanoid-robots/"},
            {"id":12,"name":"DYNA-1","org":"Dyna Robotics","date":"Apr 2025","category":"VLA","backbone":"Not disclosed","params":"N/A","decoder":"N/A","speed":"N/A","data":"Not disclosed","insight":"Claimed RL-from-deployment paradigm. Company reports continuous self-improvement from autonomous operation (99.4% success on napkin folding over 24 hrs in company demo). Technical architecture undisclosed.","paper_link":"","website_link":"","github_link":"","blog_link":"https://www.dynarobotics.ai/"},
            {"id":13,"name":"π0.5","org":"Physical Intelligence","date":"Apr 2025","category":"VLA","backbone":"PaliGemma + Action Expert","params":"3.3B","decoder":"Flow Matching","speed":"50 Hz","data":"Multi-modal: Mobile manipulation, multiple environments, multiple embodiments, web data, high-level subtask prediction","insight":"First VLA demonstrating open-world generalization to unseen homes via heterogeneous co-training. Cross-embodiment + web data + high-level subtask prediction enables transfer without target-domain scaling. Two-stage discrete→flow matching training recipe.","paper_link":"https://arxiv.org/abs/2504.16054","website_link":"https://www.physicalintelligence.company/blog/pi05","github_link":"https://github.com/Physical-Intelligence/openpi","blog_link":""},
            {"id":14,"name":"GR00T N1.5","org":"NVIDIA","date":"Jun 2025","category":"VLA","backbone":"Eagle-2.5 VLM","params":"3B","decoder":"Flow Matching","speed":"—","data":"GR-1 teleop, Open X-Embodiment, simulated GR-1 (DexMG), DreamGen neural trajectories, AgiBot-Beta","insight":"Freezes VLM during training (vs fine-tuned in GR00T N1) to preserve pretrained knowledge. FLARE world-modeling objective enables learning from human ego videos. GR00T-Dreams synthetic data generation (36h vs 3 months manual). Language following: 46.6% → 93.3%.","paper_link":"https://arxiv.org/abs/2506.10829","website_link":"","github_link":"https://github.com/NVIDIA-Omniverse/Isaac-GR00T","blog_link":"https://developer.nvidia.com/blog/advances-in-humanoid-foundation-models-with-gr00t-n1-5/"},
            {"id":15,"name":"V-JEPA 2","org":"Meta FAIR","date":"Jun 2025","category":"Predictive World Model","backbone":"JEPA","params":"1.5B","decoder":"—","speed":"4 FPS","data":"1M+ hours internet video; ~62 hrs robot data for action-conditioned variant","insight":"Non-generative world model predicting in latent space (vs pixel generation). Extreme data efficiency: 1M+ hrs internet video pretraining, only ~62 hrs robot data for action-conditioned variant. Enables zero-shot planning via MPC (65-80% pick-and-place).","paper_link":"https://arxiv.org/abs/2506.09985","website_link":"","github_link":"https://github.com/facebookresearch/jepa","blog_link":"https://ai.meta.com/blog/v-jepa-2-world-model-learning-video-data/"},
            {"id":16,"name":"Genie 3","org":"Google DeepMind","date":"Aug 2025","category":"Video World Model","backbone":"Genie 3","params":"11B","decoder":"—","speed":"24 FPS","data":"Large-scale video data","insight":"First real-time (24 FPS) interactive world model at 720p. 1-minute visual memory enables consistent multi-minute exploration. Learns physics from observation without explicit engine. Supports promptable world events and SIMA agent training.","paper_link":"https://arxiv.org/abs/2504.10458","website_link":"","github_link":"","blog_link":"https://deepmind.google/blog/genie-3-world-models-for-training-agents/"},
            {"id":17,"name":"MolmoAct","org":"Ai2","date":"Aug 2025","category":"VLA","backbone":"Qwen2.5-7B + SigLIP2","params":"7B","decoder":"Discrete Token Actions","speed":"—","data":"Pre-train: OXE subset + multimodal reasoning (26.3M samples). Mid-train: MolmoAct Dataset (10k trajectories, 93 tasks, Franka arm)","insight":"Spatial reasoning hierarchy (vs language-based hierarchy in RT-H, π0.5). 3-stage pipeline: depth perception → trajectory traces (editable waypoints) → actions. Steerable via language OR visual annotation. Fully open-source. 70.5% zero-shot SimplerEnv.","paper_link":"https://arxiv.org/abs/2508.05506","website_link":"https://molmoact.github.io/","github_link":"https://github.com/allenai/molmo-act","blog_link":""},
            {"id":18,"name":"Gemini Robotics","org":"Google DeepMind","date":"Sep 2025","category":"VLA","backbone":"Gemini 2.5","params":"—","decoder":"—","speed":">5 Hz","data":"Multi-embodiment robot data (ALOHA, Bi-arm Franka, Apollo humanoid) + internet text/image/video; thousands of diverse tasks","insight":"First multi-embodiment VLA with Motion Transfer (zero-shot skill transfer across robots without retraining). \"Thinking VLA\" mode adds multi-step reasoning before action (cf. RT-H language motions, MolmoAct spatial reasoning). Paired with GR-ER 1.5 for agentic system.","paper_link":"","website_link":"","github_link":"","blog_link":"https://deepmind.google/blog/gemini-robotics/"},
            {"id":19,"name":"GR00T N1.6","org":"NVIDIA","date":"Sep 2025","category":"VLA","backbone":"Cosmos-Reason-2B VLM","params":"3B","decoder":"Flow Matching","speed":"27 Hz","data":"GR00T N1.5 data plus several thousand hours of teleoperated data cross-embodiment","insight":"Incremental scaling: 2x larger DiT (32 vs 16 layers), unfreezes top 4 VLM layers (removes adapter). State-relative action prediction for smoother motion. Improved cross-embodiment loco-manipulation and bimanual coordination.","paper_link":"","website_link":"","github_link":"https://github.com/NVIDIA-Omniverse/Isaac-GR00T","blog_link":"https://developer.nvidia.com/blog/advancing-humanoid-robot-development-with-nvidia-gr00t-n1-6/"},
            {"id":20,"name":"Cosmos-Predict2.5","org":"NVIDIA","date":"Oct 2025","category":"Video World Model","backbone":"DiT","params":"2B-14B","decoder":"Flow Matching","speed":"—","data":"200M curated video clips; RL post-training","insight":"Unified world model (Text2World, Image2World, Video2World). RL post-training improves temporal consistency. Key infrastructure for synthetic data pipelines enables GR00T-Dreams and similar approaches. Supports 30s video via autoregressive sliding window.","paper_link":"","website_link":"","github_link":"https://github.com/NVIDIA/Cosmos","blog_link":"https://www.nvidia.com/en-us/ai/cosmos/"},
            {"id":21,"name":"VLA-0","org":"NVIDIA / UW","date":"Oct 2025","category":"VLA","backbone":"Qwen2.5-VL-3B-Instruct","params":"3B","decoder":"Discrete Action Tokens (text)","speed":"4-6 Hz","data":"LIBERO task demonstrations (no large-scale robot pretraining)","insight":"Counter-trend to action decoder complexity. Represents actions as pure text integers with zero architectural modifications. No special tokens, no diffusion/flow matching, no vocabulary changes. Questions whether complex action decoders are necessary.","paper_link":"https://arxiv.org/abs/2510.14768","website_link":"https://vla-0.github.io/","github_link":"https://github.com/vla-0/vla-0","blog_link":""},
            {"id":22,"name":"OpenVLA-OFT","org":"UC Berkeley","date":"Nov 2025","category":"VLA","backbone":"Prismatic-7B","params":"7B","decoder":"L1 Regression","speed":"25 Hz","data":"Fine-tuned on task-specific data; base OpenVLA pretrained on 970K episodes from Open X-Embodiment","insight":"Fine-tuning recipe: 3 Hz → 25 Hz via parallel decoding and action chunking. Switches from discrete tokens to continuous L1 regression (simpler than diffusion/flow matching). L1's implicit regularization filters noisy behaviors that diffusion would reproduce. Part of late-2025 simplicity counter-trend (cf. VLA-0).","paper_link":"https://arxiv.org/abs/2502.19645","website_link":"https://openvla-oft.github.io/","github_link":"https://github.com/moojink/openvla-oft","blog_link":""},
            {"id":23,"name":"GEN-0","org":"Generalist AI","date":"Nov 2025","category":"VLA","backbone":"—","params":"10B","decoder":"Diffusion + Action Tokens","speed":"—","data":"270,000+ hours real-world manipulation data; growing at 10,000 hours/week; collected from 1,000s of homes, warehouses, workplaces globally","insight":"Reports scaling laws for real-world robot data in internal experiments (270k+ hrs, growing 10k hrs/week). Identifies 7B parameter threshold in their evaluations. Claims largest real-world robot data collection but data not publicly released.","paper_link":"","website_link":"","github_link":"","blog_link":"https://generalist.ai/"},
            {"id":24,"name":"π0.6","org":"Physical Intelligence","date":"Nov 2025","category":"VLA","backbone":"Gemma3 4B + SigLIP (400M)","params":"~5.3B","decoder":"Flow Matching","speed":"50 Hz","data":"Cross-embodiment in-house data; external data sources; mobile & non-mobile home environment data; high-level subtask prediction data; multi-modal web data (bounding box, keypoint prediction)","insight":"Upgraded backbone (SigLIP + Gemma3 4B). Knowledge Insulation preserves pretrained capabilities during fine-tuning. RL variant (π*0.6) uses RECAP for learning from autonomous experience (cf. DYNA-1) to double throughput, half failure rates.","paper_link":"","website_link":"https://www.physicalintelligence.company/blog/pi06","github_link":"https://github.com/Physical-Intelligence/openpi","blog_link":""},
            {"id":25,"name":"mimic-video","org":"mimic robotics / ETH Zurich / Microsoft","date":"Dec 2025","category":"VAM","backbone":"Cosmos-Predict2 (2B DiT)","params":"~2B","decoder":"Flow Matching","speed":"—","data":"Video backbone: Cosmos pretrain + robotics video fine-tune (LoRA). Action decoder: task demos (50/task LIBERO; 1.5-2h real-world bimanual)","insight":"Establishes Video-Action Model (VAM) paradigm grounding policies in video world model latents instead of VLM. Partial denoising extracts visual action plans without full video generation. IDM isolates low-level control from dynamics learning. 10x sample efficiency vs VLAs. May represent paradigm shift from VLA to VAM.","paper_link":"https://arxiv.org/abs/2512.15692","website_link":"https://mimic-video.github.io/","github_link":"","blog_link":""},
            {"id":26,"name":"1XWM","org":"1X","date":"Jan 2026","category":"VAM","backbone":"Generative Video Model","params":"14B","decoder":"IDM (Flow Matching)","speed":"~12s/action","data":"Web-scale video → 900 hrs ego human → 70 hrs NEO robot; IDM: 400 hrs robot data","insight":"Humanoid embodiment congruence enables direct transfer from internet human video. Data pipeline: web video → 900 hrs ego human → 70 hrs robot. Only 400 hrs robot data for IDM. Qualitative demos show zero-shot generalization; quantitative success rates limited in available documentation.","paper_link":"","website_link":"","github_link":"","blog_link":"https://www.1x.tech/discover/world-model-self-learning"}
        ];

        // ============================================
        // DATA LOADING
        // ============================================
        
        let allModels = [];

        async function loadData() {
            if (USE_EMBEDDED_DATA) {
                console.log('Using embedded data');
                allModels = EMBEDDED_DATA;
                return allModels;
            }

            try {
                // Fetch from Google Sheets
                const url = `https://docs.google.com/spreadsheets/d/${GOOGLE_SHEET_ID}/gviz/tq?tqx=out:json&sheet=${SHEET_NAME}`;
                const response = await fetch(url);
                const text = await response.text();
                
                // Parse the JSONP response
                const json = JSON.parse(text.substring(47).slice(0, -2));
                const rows = json.table.rows;
                const cols = json.table.cols;
                
                // Convert to array of objects
                allModels = rows.slice(1).map((row, index) => {
                    const obj = { id: index + 1 };
                    cols.forEach((col, i) => {
                        const key = rows[0].c[i]?.v || col.label;
                        obj[key.toLowerCase().replace(/ /g, '_')] = row.c[i]?.v || '';
                    });
                    return obj;
                });
                
                return allModels;
            } catch (error) {
                console.error('Error loading from Google Sheets:', error);
                console.log('Falling back to embedded data');
                allModels = EMBEDDED_DATA;
                return allModels;
            }
        }

        // ============================================
        // RENDERING
        // ============================================

        function getCategoryClass(category) {
            if (category.includes('VLA') && !category.includes('VAM')) return 'vla';
            if (category.includes('VAM')) return 'vam';
            if (category.includes('World Model')) return 'world-model';
            if (category.includes('Planner')) return 'planner';
            return 'vla';
        }

        function renderStats(models) {
            document.getElementById('total-models').textContent = models.length;
            
            const orgs = new Set(models.map(m => m.org));
            document.getElementById('total-orgs').textContent = orgs.size;
            
            const dates = models.map(m => m.date);
            const first = dates[0];
            const last = dates[dates.length - 1];
            document.getElementById('date-range').textContent = `${first.split(' ')[1] || '2022'}-${last.split(' ')[1] || '2026'}`;
        }

        function renderTimeline(models) {
            const timeline = document.getElementById('timeline');
            
            // Group by year
            const byYear = {};
            models.forEach(m => {
                const year = m.date.split(' ')[1] || '2022';
                if (!byYear[year]) byYear[year] = [];
                byYear[year].push(m);
            });
            
            timeline.innerHTML = Object.entries(byYear).map(([year, yearModels]) => `
                <div class="timeline-year">
                    <div class="timeline-year-label">${year}</div>
                    <div class="timeline-dots">
                        ${yearModels.map(m => `
                            <div class="timeline-dot ${getCategoryClass(m.category)}" 
                                 data-name="${m.name}"
                                 onclick="openModal(${m.id})"></div>
                        `).join('')}
                    </div>
                </div>
            `).join('');
        }

        function renderCards(models) {
            const grid = document.getElementById('cards-grid');
            
            if (models.length === 0) {
                grid.innerHTML = '<div class="error"><div class="error-title">No models found</div><div class="error-message">Try adjusting your filters</div></div>';
                return;
            }
            
            grid.innerHTML = models.map(m => `
                <div class="model-card ${getCategoryClass(m.category)}" onclick="openModal(${m.id})">
                    <div class="card-header">
                        <div>
                            <div class="model-name">${m.name}</div>
                            <div class="model-org">${m.org}</div>
                        </div>
                        <div class="model-date">${m.date}</div>
                    </div>
                    <div class="card-meta">
                        <span class="meta-tag category">${m.category}</span>
                        ${m.params && m.params !== '—' && m.params !== 'N/A' ? `<span class="meta-tag">${m.params} params</span>` : ''}
                        ${m.decoder && m.decoder !== '—' && m.decoder !== 'N/A' ? `<span class="meta-tag">${m.decoder}</span>` : ''}
                        ${m.speed && m.speed !== '—' && m.speed !== 'N/A' ? `<span class="meta-tag">${m.speed}</span>` : ''}
                    </div>
                    <div class="card-insight">${m.insight}</div>
                    <div class="card-links">
                        ${m.paper_link ? `<a href="${m.paper_link}" class="card-link" target="_blank" onclick="event.stopPropagation()">Paper</a>` : ''}
                        ${m.website_link ? `<a href="${m.website_link}" class="card-link" target="_blank" onclick="event.stopPropagation()">Website</a>` : ''}
                        ${m.github_link ? `<a href="${m.github_link}" class="card-link" target="_blank" onclick="event.stopPropagation()">GitHub</a>` : ''}
                        ${m.blog_link ? `<a href="${m.blog_link}" class="card-link" target="_blank" onclick="event.stopPropagation()">Blog</a>` : ''}
                    </div>
                </div>
            `).join('');
        }

        // ============================================
        // MODAL
        // ============================================

        function openModal(id) {
            const model = allModels.find(m => m.id === id);
            if (!model) return;
            
            const content = document.getElementById('modal-content');
            content.innerHTML = `
                <div class="modal-header">
                    <div class="model-date" style="display: inline-block; margin-bottom: 0.5rem;">${model.date}</div>
                    <h2 class="modal-title">${model.name}</h2>
                    <p class="modal-subtitle">${model.org}</p>
                </div>
                
                <div class="modal-section">
                    <div class="modal-section-title">Key Insight</div>
                    <div class="modal-section-content">${model.insight}</div>
                </div>
                
                <div class="modal-section">
                    <div class="modal-section-title">Technical Specs</div>
                    <div class="modal-specs">
                        <div class="spec-item">
                            <div class="spec-label">Category</div>
                            <div class="spec-value">${model.category}</div>
                        </div>
                        <div class="spec-item">
                            <div class="spec-label">Parameters</div>
                            <div class="spec-value">${model.params || '—'}</div>
                        </div>
                        <div class="spec-item">
                            <div class="spec-label">Backbone</div>
                            <div class="spec-value">${model.backbone || '—'}</div>
                        </div>
                        <div class="spec-item">
                            <div class="spec-label">Action Decoder</div>
                            <div class="spec-value">${model.decoder || '—'}</div>
                        </div>
                        <div class="spec-item">
                            <div class="spec-label">Inference Speed</div>
                            <div class="spec-value">${model.speed || '—'}</div>
                        </div>
                    </div>
                </div>
                
                <div class="modal-section">
                    <div class="modal-section-title">Training Data</div>
                    <div class="modal-section-content">${model.data || '—'}</div>
                </div>
                
                <div class="modal-section">
                    <div class="modal-section-title">Links</div>
                    <div class="card-links" style="margin-top: 0.5rem;">
                        ${model.paper_link ? `<a href="${model.paper_link}" class="card-link" target="_blank">Paper</a>` : ''}
                        ${model.website_link ? `<a href="${model.website_link}" class="card-link" target="_blank">Website</a>` : ''}
                        ${model.github_link ? `<a href="${model.github_link}" class="card-link" target="_blank">GitHub</a>` : ''}
                        ${model.blog_link ? `<a href="${model.blog_link}" class="card-link" target="_blank">Blog</a>` : ''}
                    </div>
                </div>
            `;
            
            document.getElementById('modal-overlay').classList.add('active');
        }

        function closeModal() {
            document.getElementById('modal-overlay').classList.remove('active');
        }

        // Close modal on overlay click
        document.getElementById('modal-overlay').addEventListener('click', (e) => {
            if (e.target === e.currentTarget) closeModal();
        });

        // Close modal on Escape key
        document.addEventListener('keydown', (e) => {
            if (e.key === 'Escape') closeModal();
        });

        // ============================================
        // FILTERING
        // ============================================

        let currentFilter = 'all';
        let currentSearch = '';

        function filterModels() {
            let filtered = allModels;
            
            // Category filter
            if (currentFilter !== 'all') {
                filtered = filtered.filter(m => m.category.includes(currentFilter));
            }
            
            // Search filter
            if (currentSearch) {
                const search = currentSearch.toLowerCase();
                filtered = filtered.filter(m => 
                    m.name.toLowerCase().includes(search) ||
                    m.org.toLowerCase().includes(search) ||
                    m.insight.toLowerCase().includes(search) ||
                    m.category.toLowerCase().includes(search) ||
                    (m.backbone && m.backbone.toLowerCase().includes(search))
                );
            }
            
            renderCards(filtered);
        }

        // Filter button handlers
        document.querySelectorAll('.filter-btn').forEach(btn => {
            btn.addEventListener('click', () => {
                document.querySelectorAll('.filter-btn').forEach(b => b.classList.remove('active'));
                btn.classList.add('active');
                currentFilter = btn.dataset.filter;
                filterModels();
            });
        });

        // Search handler
        document.getElementById('search').addEventListener('input', (e) => {
            currentSearch = e.target.value;
            filterModels();
        });

        // ============================================
        // INITIALIZATION
        // ============================================

        async function init() {
            try {
                await loadData();
                renderStats(allModels);
                renderTimeline(allModels);
                renderCards(allModels);
            } catch (error) {
                console.error('Initialization error:', error);
                document.getElementById('cards-grid').innerHTML = `
                    <div class="error">
                        <div class="error-title">Failed to load data</div>
                        <div class="error-message">${error.message}</div>
                    </div>
                `;
            }
        }

        init();
    </script>
</body>
</html>
